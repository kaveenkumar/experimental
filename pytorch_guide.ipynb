# Intro

[PyTorch](https://pytorch.org/) is a very powerful machine learning framework. Central to PyTorch are [tensors](https://pytorch.org/docs/stable/tensors.html), a generalization of matrices to higher ranks. One intuitive example of a tensor is an image with three color channels: A 3-channel (red, green, blue) image which is 64 pixels wide and 64 pixels tall is a $3\times64\times64$ tensor. You can access the PyTorch framework by writing `import torch` near the top of your code, along with all of your other import statements.

This guide will help introduce you to the functionality of PyTorch, but don't worry too much about memorizing it: the assignments will link to relevant documentation where necessary.


```python
import torch
```

          # Why PyTorch?

One important question worth asking is, why is PyTorch being used for this course? There is a great breakdown by [the Gradient](https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/) looking at the state of machine learning frameworks today. In part, as highlighted by the article, PyTorch is generally more pythonic than alternative frameworks, easier to debug, and is the most-used language in machine learning research by a large and growing margin. While PyTorch's primary alternative, Tensorflow, has attempted to integrate many of PyTorch's features, Tensorflow's implementations come with some inherent limitations highlighted in the article.

Notably, while PyTorch's industry usage has grown, Tensorflow is still (for now) a slight favorite in industry. In practice, the features that make PyTorch attractive for research also make it attractive for education, and the general trend of machine learning research and practice to PyTorch makes it the more proactive choice. 

# Tensor Properties
One way to create tensors from a list or an array is to use `torch.Tensor`. It'll be used to set up examples in this notebook, but you'll never need to use it in the course - in fact, if you find yourself needing it, that's probably not the correct answer. 


```python
example_tensor = torch.Tensor(
    [
     [[1, 2], [3, 4]], 
     [[5, 6], [7, 8]], 
     [[9, 0], [1, 2]]
    ]
)
```

You can view the tensor in the notebook by simple printing it out (though some larger tensors will be cut off)


```python
example_tensor
```




    tensor([[[1., 2.],
             [3., 4.]],
    
            [[5., 6.],
             [7., 8.]],
    
            [[9., 0.],
             [1., 2.]]])


## Tensor Properties: Device

One important property is the device of the tensor - throughout this notebook you'll be sticking to tensors which are on the CPU. However, throughout the course you'll also be using tensors on GPU (that is, a graphics card which will be provided for you to use for the course). To view the device of the tensor, all you need to write is `example_tensor.device`. To move a tensor to a new device, you can write `new_tensor = example_tensor.to(device)` where device will be either `cpu` or `cuda`.


```python
example_tensor.device
```




    device(type='cpu')



## Tensor Properties: Shape

And you can get the number of elements in each dimension by printing out the tensor's shape, using `example_tensor.shape`, something you're likely familiar with if you've used numpy. For example, this tensor is a $3\times2\times2$ tensor, since it has 3 elements, each of which are $2\times2$. 


```python
example_tensor.shape
```




    torch.Size([3, 2, 2])



You can also get the size of a particular dimension $n$ using `example_tensor.shape[n]` or equivalently `example_tensor.size(n)`


```python
print("shape[0] =", example_tensor.shape[0])
print("size(1) =", example_tensor.size(1))
```

    shape[0] = 3
    size(1) = 2


Finally, it is sometimes useful to get the number of dimensions (rank) or the number of elements, which you can do as follows


```python
print("Rank =", len(example_tensor.shape))
print("Number of elements =", example_tensor.numel())
```

    Rank = 3
    Number of elements = 12

# Indexing Tensors

As with numpy, you can access specific elements or subsets of elements of a tensor. To access the $n$-th element, you can simply write `example_tensor[n]` - as with Python in general, these dimensions are 0-indexed. 


```python
example_tensor[1]
```




    tensor([[5., 6.],
            [7., 8.]])



In addition, if you want to access the $j$-th dimension of the $i$-th example, you can write `example_tensor[i, j]`


```python
example_tensor[1, 1, 0]
```




    tensor(7.)



Note that if you'd like to get a Python scalar value from a tensor, you can use `example_scalar.item()`


```python
example_scalar = example_tensor[1, 1, 0]
example_scalar.item()
```




    7.0



In addition, you can index into the ith element of a column by using `x[:, i]`. For example, if you want the top-left element of each element in `example_tensor`, which is the `0, 0` element of each matrix, you can write:


```python
example_tensor[:, 0, 0]
```




    tensor([1., 5., 9.])



# Initializing Tensors

There are many ways to create new tensors in PyTorch, but in this course, the most important ones are: 

[`torch.ones_like`](https://pytorch.org/docs/master/generated/torch.ones_like.html): creates a tensor of all ones with the same shape and device as `example_tensor`.


```python
torch.ones_like(example_tensor)
```




    tensor([[[1., 1.],
             [1., 1.]],
    
            [[1., 1.],
             [1., 1.]],
    
            [[1., 1.],
             [1., 1.]]])



[`torch.zeros_like`](https://pytorch.org/docs/master/generated/torch.zeros_like.html): creates a tensor of all zeros with the same shape and device as `example_tensor`


```python
torch.zeros_like(example_tensor)
```




    tensor([[[0., 0.],
             [0., 0.]],
    
            [[0., 0.],
             [0., 0.]],
    
            [[0., 0.],
             [0., 0.]]])



[`torch.randn_like`](https://pytorch.org/docs/stable/generated/torch.randn_like.html): creates a tensor with every element sampled from a [Normal (or Gaussian) distribution](https://en.wikipedia.org/wiki/Normal_distribution) with the same shape and device as `example_tensor`



```python
torch.randn_like(example_tensor)
```




    tensor([[[-0.3675,  0.2242],
             [-0.3378, -1.0944]],
    
            [[ 1.5371,  0.7701],
             [-0.1490, -0.0928]],
    
            [[ 0.3270,  0.4642],
             [ 0.1494,  0.1283]]])



Sometimes (though less often than you'd expect), you might need to initialize a tensor knowing only the shape and device, without a tensor for reference for `ones_like` or `randn_like`. In this case, you can create a $2x2$ tensor as follows:


```python
torch.randn(2, 2, device='cpu') # Alternatively, for a GPU tensor, you'd use device='cuda'
```




    tensor([[ 0.2235, -1.8912],
            [-1.2873,  0.7405]])


# Basic Functions

There are a number of basic functions that you should know to use PyTorch - if you're familiar with numpy, all commonly-used functions exist in PyTorch, usually with the same name. You can perform element-wise multiplication / division by a scalar $c$ by simply writing `c * example_tensor`, and element-wise addition / subtraction by a scalar by writing `example_tensor + c`

Note that most operations are not in-place in PyTorch, which means that they don't change the original variable's data (However, you can reassign the same variable name to the changed data if you'd like, such as `example_tensor = example_tensor + 1`)


```python
(example_tensor - 5) * 2
```




    tensor([[[ -8.,  -6.],
             [ -4.,  -2.]],
    
            [[  0.,   2.],
             [  4.,   6.]],
    
            [[  8., -10.],
             [ -8.,  -6.]]])



You can calculate the mean or standard deviation of a tensor using [`example_tensor.mean()`](https://pytorch.org/docs/stable/generated/torch.mean.html) or [`example_tensor.std()`](https://pytorch.org/docs/stable/generated/torch.std.html). 


```python
print("Mean:", example_tensor.mean())
print("Stdev:", example_tensor.std())
```

    Mean: tensor(4.)
    Stdev: tensor(2.9848)


You might also want to find the mean or standard deviation along a particular dimension. To do this you can simple pass the number corresponding to that dimension to the function. For example, if you want to get the average $2\times2$ matrix of the $3\times2\times2$ `example_tensor` you can write:


```python
example_tensor.mean(0)

# Equivalently, you could also write:
# example_tensor.mean(dim=0)
# example_tensor.mean(axis=0)
# torch.mean(example_tensor, 0)
# torch.mean(example_tensor, dim=0)
# torch.mean(example_tensor, axis=0)
```




    tensor([[5.0000, 2.6667],
            [3.6667, 4.6667]])



PyTorch has many other powerful functions but these should be all of PyTorch functions you need for this course outside of its neural network module (`torch.nn`).

# PyTorch Neural Network Module (`torch.nn`)

PyTorch has a lot of powerful classes in its `torch.nn` module (Usually, imported as simply `nn`). These classes allow you to create a new function which transforms a tensor in specific way, often retaining information when called multiple times.


```python
import torch.nn as nn
```

## `nn.Linear`

To create a linear layer, you need to pass it the number of input dimensions and the number of output dimensions. The linear object initialized as `nn.Linear(10, 2)` will take in a $n\times10$ matrix and return an $n\times2$ matrix, where all $n$ elements have had the same linear transformation performed. For example, you can initialize a linear layer which performs the operation $Ax + b$, where $A$ and $b$ are initialized randomly when you generate the [`nn.Linear()`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) object. 


```python
linear = nn.Linear(10, 2)
example_input = torch.randn(3, 10)
example_output = linear(example_input)
example_output
```




    tensor([[ 0.2900, -0.5352],
            [ 0.4298,  0.4173],
            [ 0.4861, -0.3332]], grad_fn=<AddmmBackward>)

## `nn.ReLU`

[`nn.ReLU()`](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) will create an object that, when receiving a tensor, will perform a ReLU activation function. This will be reviewed further in lecture, but in essence, a ReLU non-linearity sets all negative numbers in a tensor to zero. In general, the simplest neural networks are composed of series of linear transformations, each followed by activation functions. 


```python
relu = nn.ReLU()
relu_output = relu(example_output)
relu_output
```




    tensor([[0.2900, 0.0000],
            [0.4298, 0.4173],
            [0.4861, 0.0000]], grad_fn=<ReluBackward0>)

